{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Ensure that you have copied the \"order_details.csv\" file to the Files section"
      ],
      "metadata": {
        "id": "SSYjB7cXEiWb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgOktagaRGEf",
        "outputId": "63bc9eda-4233-4c19-9efc-ded374cf9c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/288.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/288.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install -q bitarray"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.1.1-bin-hadoop3.2\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "FILE_PATH = \"order_details.csv\""
      ],
      "metadata": {
        "id": "iPW2W6MFRgT2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "BO2CmSTdRloq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"PCY\")\n",
        "sc.setLogLevel(\"WARN\")"
      ],
      "metadata": {
        "id": "I5IHL4MkRmTJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to PCY Algorithm\n",
        "\n",
        "The PCY (Park-Chen-Yu) algorithm is an efficient method for finding frequent itemsets in large datasets. It belongs to the family of Apriori-based algorithms, which are widely used for association rule mining.\n",
        "\n",
        "## Algorithm Overview\n",
        "\n",
        "1. **Pass 1: Counting Frequencies**\n",
        "   - Count occurrences of each item in baskets.\n",
        "   - Create hash buckets for pairs and count their occurrences.\n",
        "\n",
        "2. **Pass 2: Filtering and Counting**\n",
        "   - Filter frequent items based on a support threshold.\n",
        "   - Create bit vectors for frequent pairs and filter candidate pairs based on frequent items and the bit vector.\n",
        "\n",
        "3. **Generating Frequent Pairs**\n",
        "   - Generate frequent pairs based on the support threshold.\n",
        "\n",
        "4. **Generating Association Rules**\n",
        "   - Calculate confidence for association rules based on frequent pairs and frequent item counts.\n",
        "\n",
        "## Results\n",
        "\n",
        "The PCY algorithm provides two main results:\n",
        "\n",
        "1. **Frequent Pairs:**\n",
        "   - Lists the pairs of items that occur frequently in the dataset along with their support count.\n",
        "\n",
        "2. **Association Rules:**\n",
        "   - Lists the association rules between itemsets with a confidence above a specified threshold. An association rule typically takes the form \"If {A} then {B}\", where A and B are itemsets, and the confidence represents the likelihood of B occurring given the occurrence of A.\n",
        "\n",
        "## Example Usage\n",
        "\n",
        "```python\n",
        "pcy = PCY(FILE_PATH, 0.1, 0.3)\n",
        "pcy.run()\n",
        "```\n",
        "\n",
        "In this example, FILE_PATH is the path to the dataset, 0.1 is the support threshold, and 0.3 is the confidence threshold.\n",
        "\n"
      ],
      "metadata": {
        "id": "s447tNcfSeZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7srsrYnSWGc6",
        "outputId": "4e5441ce-876c-4ea3-9999-98b78ada0eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of baskets: 77\n",
            "\n",
            "\n",
            "Frequent Pairs: \n",
            "+---------------------------------------------------------------------------------------+----+\n",
            "|frequent_pairs                                                                         |freq|\n",
            "+---------------------------------------------------------------------------------------+----+\n",
            "|Banana Fish - Tập 20, Đập Chắn Thái Bình Dương                                         |13  |\n",
            "|Trump - Đừng Bao Giờ Bỏ Cuộc, Đường Xa Nắng Mới                                        |8   |\n",
            "|Đường Xa Nắng Mới, Đập Chắn Thái Bình Dương                                            |8   |\n",
            "|Viết Sao Cho Hay, Đập Chắn Thái Bình Dương                                             |10  |\n",
            "|Trump - Đừng Bao Giờ Bỏ Cuộc, Đập Chắn Thái Bình Dương                                 |9   |\n",
            "|Attack On Titan - Tập 5, Đập Chắn Thái Bình Dương                                      |9   |\n",
            "|Attack On Titan - Tập 5, Banana Fish - Tập 20                                          |11  |\n",
            "|Banana Fish - Tập 20, Trump - Đừng Bao Giờ Bỏ Cuộc                                     |10  |\n",
            "|Banana Fish - Tập 20, Ninja Rantaro - Tập 26                                           |8   |\n",
            "|Banana Fish - Tập 20, Thấu Hiểu Tiếp Thị Từ A Đến Z - 80 Khái Niệm Nhà Quản Lý Cần Biết|8   |\n",
            "+---------------------------------------------------------------------------------------+----+\n",
            "\n",
            "\n",
            "Association Rules: \n",
            "+--------------------+--------------------+-------------------+\n",
            "|          antecedent|          consequent|         confidence|\n",
            "+--------------------+--------------------+-------------------+\n",
            "|Banana Fish - Tập 20|Đập Chắn Thái Bìn...| 0.3170731707317073|\n",
            "|Đập Chắn Thái Bìn...|Banana Fish - Tập 20|0.41935483870967744|\n",
            "|Trump - Đừng Bao ...|   Đường Xa Nắng Mới|0.36363636363636365|\n",
            "|   Đường Xa Nắng Mới|Trump - Đừng Bao ...| 0.5333333333333333|\n",
            "|   Đường Xa Nắng Mới|Đập Chắn Thái Bìn...| 0.5333333333333333|\n",
            "|    Viết Sao Cho Hay|Đập Chắn Thái Bìn...|0.47619047619047616|\n",
            "|Đập Chắn Thái Bìn...|    Viết Sao Cho Hay| 0.3225806451612903|\n",
            "|Trump - Đừng Bao ...|Đập Chắn Thái Bìn...| 0.4090909090909091|\n",
            "|Attack On Titan -...|Đập Chắn Thái Bìn...|0.47368421052631576|\n",
            "|Attack On Titan -...|Banana Fish - Tập 20| 0.5789473684210527|\n",
            "|Trump - Đừng Bao ...|Banana Fish - Tập 20|0.45454545454545453|\n",
            "|Ninja Rantaro - T...|Banana Fish - Tập 20| 0.4444444444444444|\n",
            "|Thấu Hiểu Tiếp Th...|Banana Fish - Tập 20|                0.8|\n",
            "+--------------------+--------------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from bitarray import bitarray\n",
        "from pyspark.sql import SQLContext\n",
        "from itertools import combinations\n",
        "\n",
        "class PCY:\n",
        "  def __init__(self, path, support_threshold, confidence_threshold):\n",
        "    self.spark = SQLContext(sc)\n",
        "    self.path = path\n",
        "    self.support_threshold = support_threshold\n",
        "    self.confidence_threshold = confidence_threshold\n",
        "\n",
        "  def read_data(self):\n",
        "    df = self.spark.read.csv(self.path, header=True, inferSchema=True)\n",
        "    baskets = df.groupBy(\"order_id\") \\\n",
        "                .agg({\"product_name\": \"collect_list\"}) \\\n",
        "                .withColumnRenamed(\"collect_list(product_name)\", \"Basket\")\n",
        "\n",
        "    self.baskets = baskets.select(\"Basket\").rdd.flatMap(lambda x: x).map(lambda x: sorted(x))\n",
        "    self.baskets_count = self.baskets.count()\n",
        "    print('Number of baskets: %i\\n' % self.baskets_count)\n",
        "\n",
        "    self.pairs = self.baskets.flatMap(lambda x: list(combinations(x, 2)))\n",
        "\n",
        "  def pass_1(self):\n",
        "    # Counting occurrences of each item in baskets\n",
        "    self.item_counts = self.baskets.flatMap(lambda x: [(item, 1) for item in x]) \\\n",
        "                                   .reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "    # Creating hash buckets for pairs and counting occurrences\n",
        "    self.buckets = self.pairs.map(lambda x: (hash(tuple(x)) % 1000 , 1)) \\\n",
        "                             .reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "  def pass_2(self):\n",
        "    support = self.support_threshold * self.baskets_count\n",
        "    # Filtering frequent items based on support threshold\n",
        "    self.frequent_items = self.item_counts.filter(lambda x: x[1] >= support).collect()\n",
        "\n",
        "    # Extracting item IDs from frequent items\n",
        "    frequent_items = list(map(lambda x: x[0],self.frequent_items))\n",
        "    # Creating bit vectors for frequent pairs\n",
        "    bitvector_list = self.buckets.map(lambda x: (x[0], 1 if x[1] >= support else 0)).collect()\n",
        "\n",
        "    bitvector = bitarray(1000)\n",
        "    bitvector.setall(0)\n",
        "\n",
        "    for item in bitvector_list:\n",
        "      bitvector[item[0]] = 1 if item[1] == 1 else 0\n",
        "\n",
        "    bitvector_list.clear()\n",
        "\n",
        "    # Filtering candidate pairs based on frequent items and bit vector\n",
        "    self.candidate_pairs = self.pairs.filter(lambda pair: pair[0] in frequent_items and pair[1] in frequent_items) \\\n",
        "                                     .filter(lambda pair: bitvector[hash(tuple(pair)) % 1000] == 1) \\\n",
        "                                     .map(lambda pair: (pair, 1)) \\\n",
        "                                     .reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "  def generate_frequent_pairs(self):\n",
        "    support = self.support_threshold * self.baskets_count\n",
        "    self.frequent_pairs = self.candidate_pairs.filter(lambda x: x[1] >= support)\n",
        "\n",
        "    if(self.frequent_pairs.count() > 0):\n",
        "      result = self.frequent_pairs.map(lambda x: (\", \".join(x[0]), x[1])) \\\n",
        "                                  .toDF(['frequent_pairs', 'freq'])\n",
        "      print('\\nFrequent Pairs: ')\n",
        "      result.show(truncate=False)\n",
        "    else:\n",
        "      print(\"There are no frequent pairs with this support threshold\")\n",
        "\n",
        "  def generate_association_rules(self):\n",
        "    frequent_items = self.frequent_items\n",
        "    confidence_threshold = self.confidence_threshold\n",
        "    def calculate_confidence(freq, freq_a):\n",
        "        return freq / freq_a\n",
        "\n",
        "    def find_item_freq(item):\n",
        "      return next((v for k, v in frequent_items if k == item), 0)\n",
        "\n",
        "    pairs = self.frequent_pairs\n",
        "\n",
        "    # Calculating confidence for association rules\n",
        "    association_rules = pairs.flatMap(lambda x: [((x[0][0], x[0][1]), (x[1],find_item_freq(x[0][0]))),\n",
        "                                                 ((x[0][1], x[0][0]), (x[1], find_item_freq(x[0][1])))]) \\\n",
        "                             .map(lambda x: (x[0], calculate_confidence(x[1][0], x[1][1]))) \\\n",
        "                             .filter(lambda x: x[1] >= confidence_threshold) \\\n",
        "                             .map(lambda x: (x[0][0], x[0][1], x[1]))\n",
        "\n",
        "    if(association_rules.count() > 0):\n",
        "      result = association_rules.toDF(['antecedent', 'consequent', 'confidence'])\n",
        "      print('\\nAssociation Rules: ')\n",
        "      result.show()\n",
        "    else:\n",
        "      print(\"There are no association rules with this confidence\")\n",
        "\n",
        "  def run(self):\n",
        "    self.read_data()\n",
        "    self.pass_1()\n",
        "    self.pass_2()\n",
        "\n",
        "    self.generate_frequent_pairs()\n",
        "    self.generate_association_rules()\n",
        "\n",
        "# Example usage:\n",
        "pcy = PCY(FILE_PATH, 0.1, 0.3)\n",
        "pcy.run()\n"
      ]
    }
  ]
}